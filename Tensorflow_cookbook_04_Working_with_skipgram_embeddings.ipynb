{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100       # 한 번에 얼마나 많은 단어를 train 할 지\n",
    "embedding_size = 100   # 단어당 embedding vector의 길이\n",
    "vocabulary_size = 2000 # 얼마나 많은 단어를 트레이닝에 쓸 것인지\n",
    "generations = 100000   # epoch 사이즈\n",
    "print_loss_every = 1000# 결과값을 1000번에 한 번 보고\n",
    "\n",
    "num_sampled = int(batch_size/2)\n",
    "window_size = 5        # 윈도우에 들어갈 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopword지정\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# test\n",
    "print_valid_every = 10000\n",
    "valid_words = ['cliche', 'love','hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\\rt-polaritydata\\rt-polarity.pos\n"
     ]
    }
   ],
   "source": [
    "save_folder_name = 'temp'\n",
    "pos_file = os.path.join(save_folder_name, 'rt-polaritydata','rt-polarity.pos')\n",
    "print(pos_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name = 'temp'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata','rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata','rt-polarity.neg')\n",
    "    \n",
    "    # 파일이 이미 있는지 확인하기\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        \n",
    "        # Save tar.gz file\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size = 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        \n",
    "        # Extract tar.gz file into temp folder\n",
    "        tar = tarfile.open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path='temp')\n",
    "        tar.close()\n",
    "    \n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    pos_data = [x.rstrip() for x in pos_data] # rstrip: 오른쪽 공백 지우기\n",
    "    \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    \n",
    "    return (texts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = data[0]\n",
    "target = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text에서 필요없는 부분을 떼어내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    \n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    texts = [' '.join(word for word in x.split() if word not in (stops)) for x in texts]\n",
    "    \n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) >2]\n",
    "texts = [x for x in texts if len(x.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gorgeously elaborate continuation lord rings trilogy huge column words cannot adequately describe cowriterdirector peter jacksons expanded vision j r r tolkiens middleearth'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장에 있는 단어들을 빈도수 기준 정렬, 등수 번호 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # Turn sentences into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    \n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    \n",
    "    # 가장 빈도수가 높은 단어들 N개를 이 리스트에 추가\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    # counter.most_common(K)을 쓰면 상위 K개의 최빈도 단어들이 아래와 같은 상태로 나옴\n",
    "    # 예) K=3일 때, [('counter',10),('words',7), ('collections',5)]\n",
    "    # 단어와 빈도로 이루어진 count 리스트를 딕셔너리 형태로 변환\n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        # 최빈도 단어들에 등수를 부여하는 과정\n",
    "        # dictionary length는 0, 1, 2, 3.... 이렇게 늘어남\n",
    "        # 아래의 예시를 보면 알 수 있음\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (참고) Dictionary 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rare', -1], ('k', 6), ('d', 4), ('a', 2), ('c', 2), ('g', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Rare': 0, 'a': 3, 'c': 4, 'd': 2, 'g': 5, 'k': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a', 'a', 'c', 'c', 'g', 'g', 'd', 'd', 'd', 'd', 'k', 'k', 'k','k', 'k', 'k']\n",
    "count1 = [['Rare',-1]]\n",
    "count1.extend(collections.Counter(a).most_common())\n",
    "print(count1)\n",
    "\n",
    "word_dict1 = {}\n",
    "for word, word_count in count1:\n",
    "    word_dict1[word] = len(word_dict1)\n",
    "\n",
    "word_dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 단어의 리스트로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    # initialize the returned data\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # word_dict에 있는 단어라면 해당 인덱스를, 없는 단어면 0번을 부여\n",
    "        for word in sentence.split(' '):\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋을 위의 함수들로 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1490, 28, 940, 205, 359]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 랜덤하게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size, method='skip-gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentences to start\n",
    "        rand_sentences = np.random.choice(sentences)\n",
    "        # Genrate consecutive windows to look at\n",
    "        window_sequences= [rand_sentences[max((ix-window_size),0):(ix+window_size)] \n",
    "                           for ix, x in enumerate(rand_sentences)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        # 19번째 줄에 대한 설명:\n",
    "        # window_sequences는 (ix-window_size에서 ix-1까지가 왼쪽 문맥, ix+1부터 ix+window_size까지가 오른쪽 문맥이다)\n",
    "        # 중심단어는 ix인 셈인데, 문제는 window_size보다 작은 ix들이 padding이 있어야만 중심단어라는데 있다.\n",
    "        # 하지만 여기서는 과감히 padding을 하지 않고, 일단 중심단어부터 뽑아내고 있다. \n",
    "        # window_sequence는 padding이 안 되는 앞 단어들에 대해서는 본인의 index를 쓰도록 하고,\n",
    "        # 중간의 단어들에 대해서는 length가 target word(1) + window_size *2의 length를 만족시키므로\n",
    "        # center word의 인덱스로 window_size를 써도 된다. (이 말이 더 어려우므로 밑의 참고를 보자.)\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip-gram': # 단어로부터 문맥단어들을 유추해내는 방식\n",
    "            batch_and_labels = [(x[y], x[:y] + x[y+1:]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it into a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y]+x[y+1:], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it into a big list of tuples\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "        \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])  # uniform한 길이만 쓰기 위해서 batch_size로 끊어낸다\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    \n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "        \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data])) # 세로로 세워서 각각의 label data를 리스트로 싼 모양\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위의 메커니즘이 이해가 안되면 참고 (특히 label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['i','am','bad'], \n",
    "             ['you','are','good'], \n",
    "             ['he','is','magnificent','as','fuck'],\n",
    "             ['are','you','dumb','as','fuck'],\n",
    "             ['how','on','earth','can','you','do','that']]\n",
    "\n",
    "window_size = 2\n",
    "np.random.seed(777)\n",
    "rand_sentences1 = np.random.choice(sentences)\n",
    "window_sequences= [rand_sentences1[max((ix-window_size),0):(ix+window_size+1)] \n",
    "                           for ix, x in enumerate(rand_sentences1)]\n",
    "label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['are', 'you', 'dumb'],\n",
       " ['are', 'you', 'dumb', 'as'],\n",
       " ['are', 'you', 'dumb', 'as', 'fuck'],\n",
       " ['you', 'dumb', 'as', 'fuck'],\n",
       " ['dumb', 'as', 'fuck']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.array([label_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것의 결과를 잘 보자. \n",
    "첫 번째와 두 번째 window sequence에서는 length가 1 + window_size*2보다 작다. \n",
    "그렇기 때문에 중심 단어는 늘 index그 자체일 수밖에 없다. \n",
    "첫 번째의 window_sequence에 대해서 index는 0,\n",
    "두 번째 window_sequence에 대해서 index는 1이다.\n",
    "하지만 sequence length가 5인 \"are you dumb as fuck\"의 경우\n",
    "window_size를 index로 하면 중심단어인 dumb이 선택된다. \n",
    "길이가 sequence length와 같은 것들은 모두 index를 2로 가질 것이라는 것은 예상할 수 있다.\n",
    "\n",
    "하지만 왜 sequence length가 5보다 작은 뒤의 window sequence들의 경우 그냥 \n",
    "window_size를 쓸 수 있는 것인가?\n",
    "그 이유는 indices가 앞에서부터 세어진다는데 있다. \n",
    "뒤쪽에 padding이 필요한 window sequence의 경우\n",
    "뒤쪽이 얼마나 짧아지든 앞쪽에서부터 세어지기 때문에 그냥 window_size를 index로\n",
    "채택할 수 있는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_and_labels = [(x[y], x[:y] + x[y+1:]) for x,y in zip(window_sequences, label_indices)]\n",
    "tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 'you'),\n",
       " ('are', 'dumb'),\n",
       " ('you', 'are'),\n",
       " ('you', 'dumb'),\n",
       " ('you', 'as'),\n",
       " ('dumb', 'are'),\n",
       " ('dumb', 'you'),\n",
       " ('dumb', 'as'),\n",
       " ('dumb', 'fuck'),\n",
       " ('as', 'you'),\n",
       " ('as', 'dumb'),\n",
       " ('as', 'fuck'),\n",
       " ('fuck', 'dumb'),\n",
       " ('fuck', 'as')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip-gram : 단어에서 문맥추정\n",
    "tuple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Embedding matrix 만들기 위한 준비과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0,1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                             stddev=1.0/np.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                    biases=nce_biases,\n",
    "                                    labels=y_target,\n",
    "                                    inputs=embed,\n",
    "                                    num_sampled=num_sampled,\n",
    "                                    num_classes=vocabulary_size))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "# Valid embeddings는 그냥 테스트를 위해 아까 만든 valid samples와 관련된 것이어서 크게 의미두지 말자\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "\n",
    "# Valid embeddings와 normalized embeddings 사이의 cosine similarity\n",
    "# 내적을 위해 transpose가 필요함. \n",
    "# valid_embeddings :      (5, embedding_size)\n",
    "# normalized_embeddings : (vocab_size, embedding_size)\n",
    "# similarity:             (5, vocab_size)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Add variable initializer\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2000)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_init = sess.run(similarity)\n",
    "sim_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf.reduce_sum에서 keep_dims의 의미와 normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[[2.0,1.0,2.0],[3.,4.,5.]],[[2.,1.,3.],[5.,6.,7.]],[[0,1.,4.],[2.,1.,4.]]])\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 1., 2.],\n",
       "        [3., 4., 5.]],\n",
       "\n",
       "       [[2., 1., 3.],\n",
       "        [5., 6., 7.]],\n",
       "\n",
       "       [[0., 1., 4.],\n",
       "        [2., 1., 4.]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 2.],\n",
       "       [3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.      ],\n",
       "       [7.071068]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(a[0]), 1,keep_dims=True))\n",
    "norm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6666667 , 0.33333334, 0.6666667 ],\n",
       "       [0.42426407, 0.56568545, 0.70710677]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = a[0]\n",
    "normalized.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 cosine similarity를 구하기 위해 norm을 구할 때, embeddings matrix의 dimension을 생각해볼 필요가 있다. 그 dimension은 [vocab_size, embed_size].\n",
    "tf.reduce_sum에서 axis를 1로 지정해주었다는 것은 세로줄끼리 연산을 한다는 것이다. 결국 결과값의 dimension은 [vocab_size, 1]이 된다. \n",
    "\n",
    "이렇게 구해진 norm으로 나눈다는 것은 위와 같이 계산됨을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNNNNNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000 : 4.061773777008057\n",
      "Loss at step 2000 : 4.1783766746521\n",
      "Loss at step 3000 : 3.935540199279785\n",
      "Loss at step 4000 : 4.181026458740234\n",
      "Loss at step 5000 : 4.602260589599609\n",
      "Loss at step 6000 : 3.6549248695373535\n",
      "Loss at step 7000 : 3.282228469848633\n",
      "Loss at step 8000 : 3.7064990997314453\n",
      "Loss at step 9000 : 3.6334218978881836\n",
      "Loss at step 10000 : 4.053454875946045\n",
      "Nearest to cliche: RARE, poignancy, films, proceedings, russian,\n",
      "Nearest to love: RARE, queen, relationships, history, thriller,\n",
      "Nearest to hate: emotionally, cloying, tribute, likely, um,\n",
      "Nearest to silly: revenge, showtime, apparently, impressive, eccentric,\n",
      "Nearest to sad: slightly, finest, natural, atmosphere, format,\n",
      "Loss at step 11000 : 3.6322875022888184\n",
      "Loss at step 12000 : 3.4331555366516113\n",
      "Loss at step 13000 : 3.484189987182617\n",
      "Loss at step 14000 : 3.7072174549102783\n",
      "Loss at step 15000 : 4.718789100646973\n",
      "Loss at step 16000 : 3.728086471557617\n",
      "Loss at step 17000 : 3.8656671047210693\n",
      "Loss at step 18000 : 3.7675397396087646\n",
      "Loss at step 19000 : 4.214008808135986\n",
      "Loss at step 20000 : 4.258507251739502\n",
      "Nearest to cliche: films, poignancy, already, words, russian,\n",
      "Nearest to love: RARE, beginning, relationships, fat, precious,\n",
      "Nearest to hate: emotionally, cloying, tribute, likely, una,\n",
      "Nearest to silly: revenge, RARE, eccentric, showtime, impressive,\n",
      "Nearest to sad: slightly, natural, finest, format, screen,\n",
      "Loss at step 21000 : 3.8158764839172363\n",
      "Loss at step 22000 : 3.748013973236084\n",
      "Loss at step 23000 : 3.476193904876709\n",
      "Loss at step 24000 : 3.844339370727539\n",
      "Loss at step 25000 : 4.298649787902832\n",
      "Loss at step 26000 : 3.698880910873413\n",
      "Loss at step 27000 : 3.8340160846710205\n",
      "Loss at step 28000 : 2.9568212032318115\n",
      "Loss at step 29000 : 3.6605401039123535\n",
      "Loss at step 30000 : 3.1810452938079834\n",
      "Nearest to cliche: films, words, poignancy, forget, RARE,\n",
      "Nearest to love: fat, relationships, style, potential, beginning,\n",
      "Nearest to hate: emotionally, likely, cloying, tribute, like,\n",
      "Nearest to silly: revenge, gory, greek, impressive, rarely,\n",
      "Nearest to sad: slightly, natural, sexual, RARE, format,\n",
      "Loss at step 31000 : 3.9953079223632812\n",
      "Loss at step 32000 : 4.2156901359558105\n",
      "Loss at step 33000 : 4.011890888214111\n",
      "Loss at step 34000 : 3.9296350479125977\n",
      "Loss at step 35000 : 4.1541242599487305\n",
      "Loss at step 36000 : 3.686089515686035\n",
      "Loss at step 37000 : 4.297887802124023\n",
      "Loss at step 38000 : 3.839840888977051\n",
      "Loss at step 39000 : 3.754349946975708\n",
      "Loss at step 40000 : 4.002704620361328\n",
      "Nearest to cliche: forget, words, films, poignancy, even,\n",
      "Nearest to love: potential, fat, RARE, precious, comingofage,\n",
      "Nearest to hate: likely, tribute, cloying, emotionally, una,\n",
      "Nearest to silly: revenge, rarely, selfindulgent, gory, greek,\n",
      "Nearest to sad: natural, slightly, sexual, ghost, finest,\n",
      "Loss at step 41000 : 4.058895587921143\n",
      "Loss at step 42000 : 3.62838077545166\n",
      "Loss at step 43000 : 3.107694149017334\n",
      "Loss at step 44000 : 4.011722564697266\n",
      "Loss at step 45000 : 3.982562780380249\n",
      "Loss at step 46000 : 3.644467353820801\n",
      "Loss at step 47000 : 3.955756902694702\n",
      "Loss at step 48000 : 3.723116159439087\n",
      "Loss at step 49000 : 3.2775418758392334\n",
      "Loss at step 50000 : 3.5165178775787354\n",
      "Nearest to cliche: even, words, performance, films, forget,\n",
      "Nearest to love: potential, precious, comingofage, fat, relationships,\n",
      "Nearest to hate: likely, tribute, RARE, cloying, una,\n",
      "Nearest to silly: revenge, rarely, RARE, selfindulgent, gory,\n",
      "Nearest to sad: natural, slightly, ghost, finest, horrible,\n",
      "Loss at step 51000 : 3.9082589149475098\n",
      "Loss at step 52000 : 3.5621395111083984\n",
      "Loss at step 53000 : 4.103659629821777\n",
      "Loss at step 54000 : 3.3416876792907715\n",
      "Loss at step 55000 : 3.8476028442382812\n",
      "Loss at step 56000 : 4.297264575958252\n",
      "Loss at step 57000 : 4.087670803070068\n",
      "Loss at step 58000 : 3.8050479888916016\n",
      "Loss at step 59000 : 3.4900336265563965\n",
      "Loss at step 60000 : 3.518838405609131\n",
      "Nearest to cliche: words, performance, level, even, screen,\n",
      "Nearest to love: potential, frontal, fat, comingofage, mad,\n",
      "Nearest to hate: likely, tribute, like, una, predictably,\n",
      "Nearest to silly: revenge, rarely, selfindulgent, gory, grows,\n",
      "Nearest to sad: natural, ghost, finest, slightly, horrible,\n",
      "Loss at step 61000 : 3.576667070388794\n",
      "Loss at step 62000 : 3.523958683013916\n",
      "Loss at step 63000 : 3.8537991046905518\n",
      "Loss at step 64000 : 4.003161430358887\n",
      "Loss at step 65000 : 3.041224956512451\n",
      "Loss at step 66000 : 3.083966016769409\n",
      "Loss at step 67000 : 3.6756420135498047\n",
      "Loss at step 68000 : 3.8231050968170166\n",
      "Loss at step 69000 : 2.815065860748291\n",
      "Loss at step 70000 : 3.810831069946289\n",
      "Nearest to cliche: words, even, screen, forget, literally,\n",
      "Nearest to love: potential, comingofage, fat, mad, loss,\n",
      "Nearest to hate: likely, tribute, like, una, predictably,\n",
      "Nearest to silly: revenge, selfindulgent, rarely, scifi, trying,\n",
      "Nearest to sad: natural, ghost, horrible, unpleasant, finest,\n",
      "Loss at step 71000 : 3.590250253677368\n",
      "Loss at step 72000 : 3.7097396850585938\n",
      "Loss at step 73000 : 3.3923587799072266\n",
      "Loss at step 74000 : 3.1161415576934814\n",
      "Loss at step 75000 : 3.768282890319824\n",
      "Loss at step 76000 : 4.1203107833862305\n",
      "Loss at step 77000 : 3.536799669265747\n",
      "Loss at step 78000 : 3.4762909412384033\n",
      "Loss at step 79000 : 3.7906594276428223\n",
      "Loss at step 80000 : 3.9554333686828613\n",
      "Nearest to cliche: even, words, performance, possibly, forget,\n",
      "Nearest to love: potential, comingofage, frontal, loss, fat,\n",
      "Nearest to hate: likely, tribute, una, thinking, predictably,\n",
      "Nearest to silly: revenge, rarely, trying, brother, selfindulgent,\n",
      "Nearest to sad: natural, unpleasant, ghost, triumph, finest,\n",
      "Loss at step 81000 : 3.6139109134674072\n",
      "Loss at step 82000 : 3.416445016860962\n",
      "Loss at step 83000 : 3.416797161102295\n",
      "Loss at step 84000 : 4.057309150695801\n",
      "Loss at step 85000 : 3.5248022079467773\n",
      "Loss at step 86000 : 4.0804948806762695\n",
      "Loss at step 87000 : 3.837383985519409\n",
      "Loss at step 88000 : 3.4159481525421143\n",
      "Loss at step 89000 : 3.718320846557617\n",
      "Loss at step 90000 : 3.4552340507507324\n",
      "Nearest to cliche: words, even, screen, possibly, forget,\n",
      "Nearest to love: RARE, potential, comingofage, mad, see,\n",
      "Nearest to hate: likely, tribute, una, thinking, shake,\n",
      "Nearest to silly: revenge, scifi, rarely, brother, exercise,\n",
      "Nearest to sad: natural, finest, unpleasant, triumph, ghost,\n",
      "Loss at step 91000 : 3.849398612976074\n",
      "Loss at step 92000 : 3.5991203784942627\n",
      "Loss at step 93000 : 3.445590019226074\n",
      "Loss at step 94000 : 3.2405893802642822\n",
      "Loss at step 95000 : 4.285735607147217\n",
      "Loss at step 96000 : 3.3845462799072266\n",
      "Loss at step 97000 : 4.13855504989624\n",
      "Loss at step 98000 : 3.5926685333251953\n",
      "Loss at step 99000 : 3.7261877059936523\n",
      "Loss at step 100000 : 3.6578073501586914\n",
      "Nearest to cliche: words, even, forget, un, possibly,\n",
      "Nearest to love: potential, comingofage, frontal, fat, mad,\n",
      "Nearest to hate: likely, tribute, shake, una, thinking,\n",
      "Nearest to silly: revenge, scifi, rarely, grows, brother,\n",
      "Nearest to sad: unpleasant, natural, existence, ingredients, ghost,\n"
     ]
    }
   ],
   "source": [
    "# Run the skip-gram model\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
    "    \n",
    "    # Run the trainstep\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "    # Return the loss\n",
    "    if (i+1)% print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "        \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1)%print_valid_every == 0:\n",
    "        # 전체 vocab 단어들과 valid examples의 원소와의 similarity를 (5,2000)에 표시한 것이 sim\n",
    "        sim = sess.run(similarity)    \n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5\n",
    "            # valid example에 있는 단어와 같은 단어는 제외해야 하므로 0부터 시작 안 함\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1] \n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                score = sim[j, nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
